{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 16:09:37.109993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating            timestamp\n",
      "0       1        2     3.5  2005-04-02 23:53:47\n",
      "1       1       29     3.5  2005-04-02 23:31:16\n",
      "2       1       32     3.5  2005-04-02 23:33:39\n",
      "3       1       47     3.5  2005-04-02 23:32:07\n",
      "4       1       50     3.5  2005-04-02 23:29:40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from helpers import *\n",
    "# Load the TSV file\n",
    "df = pd.read_csv('ml-32m/ratings.csv')\n",
    "\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './MovieSummaries/'\n",
    "#paths to files\n",
    "plot_summaries_path = data_folder + 'plot_summaries.txt'\n",
    "movie_metadata_path = data_folder + 'movie.metadata.tsv'\n",
    "character_metadata_path = data_folder + 'character.metadata.tsv'\n",
    "\n",
    "# load the data\n",
    "# 1. Plot summaries data\n",
    "plot_summaries_df = pd.read_csv(plot_summaries_path, delimiter='\\t', names=['wikipedia_movie_id', 'plot_summary'], \n",
    "                                 encoding='utf-8')\n",
    "\n",
    "# 2. Movie metadata\n",
    "movie_metadata_df = pd.read_csv(movie_metadata_path, delimiter='\\t', names=['wikipedia_movie_id', 'freebase_movie_id', \n",
    "                                                                            'movie_name', 'release_date', 'box_office_revenue',\n",
    "                                                                            'runtime', 'languages', 'countries', 'genres'], \n",
    "                                 encoding='utf-8')\n",
    "\n",
    "# 3. Character metadata\n",
    "character_metadata_df = pd.read_csv(character_metadata_path, delimiter='\\t', names=['wikipedia_movie_id', 'freebase_movie_id', 'release_date', 'character_name', \n",
    "                                                                                    'actor_dob', 'actor_gender', 'actor_height', 'actor_ethnicity', 'actor_name', \n",
    "                                                                                    'actor_age_at_release', 'freebase_character_actor_map_id', 'freebase_character_id', \n",
    "                                                                                    'freebase_actor_id'], \n",
    "                                     encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('ml-32m/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata_df['movie_name_formatted'] = movie_metadata_df['movie_name'].str.lower().str.strip()\n",
    "movies['title_format'] = movies['title'].str[:-6].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>title_format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>grumpier old men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>waiting to exhale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>father of the bride part ii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres                 title_format  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy                    toy story  \n",
       "1                   Adventure|Children|Fantasy                      jumanji  \n",
       "2                               Comedy|Romance             grumpier old men  \n",
       "3                         Comedy|Drama|Romance            waiting to exhale  \n",
       "4                                       Comedy  father of the bride part ii  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common movies: 12318\n"
     ]
    }
   ],
   "source": [
    "# calculate the number of common movies between the two datasets\n",
    "\n",
    "common_movies = set(movie_metadata_df['movie_name_formatted']).intersection(set(movies['title_format']))\n",
    "print('Number of common movies:', len(common_movies))\n",
    "\n",
    "# merge the two datasets\n",
    "\n",
    "merged_df = pd.merge(movies, movie_metadata_df, left_on='title_format', right_on='movie_name_formatted', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres_x</th>\n",
       "      <th>title_format</th>\n",
       "      <th>wikipedia_movie_id</th>\n",
       "      <th>freebase_movie_id</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>release_date</th>\n",
       "      <th>box_office_revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>languages</th>\n",
       "      <th>countries</th>\n",
       "      <th>genres_y</th>\n",
       "      <th>movie_name_formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>53085</td>\n",
       "      <td>/m/0dyb1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995-11-19</td>\n",
       "      <td>361958736.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/0556j8\": \"Buddy film\", \"/m/03k9fj\": \"Adve...</td>\n",
       "      <td>toy story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>jumanji</td>\n",
       "      <td>3700174</td>\n",
       "      <td>/m/09w353</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>{\"/m/064_8sq\": \"French Language\", \"/m/02h40lc\"...</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\", \"/m/...</td>\n",
       "      <td>{\"/m/01jfsb\": \"Thriller\", \"/m/0hj3n2k\": \"Fanta...</td>\n",
       "      <td>jumanji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>grumpier old men</td>\n",
       "      <td>1934035</td>\n",
       "      <td>/m/0676dr</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>71518503.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>{\"/m/02bjrlw\": \"Italian Language\", \"/m/02h40lc...</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/06cvj\": \"Romantic comedy\", \"/m/0556j8\": \"...</td>\n",
       "      <td>grumpier old men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>waiting to exhale</td>\n",
       "      <td>972970</td>\n",
       "      <td>/m/03vny7</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/0hj3n0w\": \"Ensemble Film\", \"/m/06w2n3t\": ...</td>\n",
       "      <td>waiting to exhale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>father of the bride part ii</td>\n",
       "      <td>3303622</td>\n",
       "      <td>/m/094g2z</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>1995-12-08</td>\n",
       "      <td>76594107.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/06qm3\": \"Screwball comedy\", \"/m/02l7c8\": ...</td>\n",
       "      <td>father of the bride part ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17557</th>\n",
       "      <td>131152</td>\n",
       "      <td>The Fat Spy (1966)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>the fat spy</td>\n",
       "      <td>1853200</td>\n",
       "      <td>/m/0613sx</td>\n",
       "      <td>The Fat Spy</td>\n",
       "      <td>1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/0gf28\": \"Parody\", \"/m/0220p9g\": \"Musical ...</td>\n",
       "      <td>the fat spy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17558</th>\n",
       "      <td>131166</td>\n",
       "      <td>WWII IN HD (2009)</td>\n",
       "      <td>(no genres listed)</td>\n",
       "      <td>wwii in hd</td>\n",
       "      <td>26321497</td>\n",
       "      <td>/m/0bbxzy2</td>\n",
       "      <td>WWII in HD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"/m/082gq\": \"War film\"}</td>\n",
       "      <td>wwii in hd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17559</th>\n",
       "      <td>131239</td>\n",
       "      <td>Three Quarter Moon (2011)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>three quarter moon</td>\n",
       "      <td>33597212</td>\n",
       "      <td>/m/0hgrd5g</td>\n",
       "      <td>Three Quarter Moon</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.0</td>\n",
       "      <td>{\"/m/02hwyss\": \"Turkish Language\", \"/m/0gtg\": ...</td>\n",
       "      <td>{\"/m/0345h\": \"Germany\"}</td>\n",
       "      <td>{\"/m/0556j8\": \"Buddy film\", \"/m/0hqxf\": \"Famil...</td>\n",
       "      <td>three quarter moon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17560</th>\n",
       "      <td>131248</td>\n",
       "      <td>Brother Bear 2 (2006)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>brother bear 2</td>\n",
       "      <td>4118248</td>\n",
       "      <td>/m/0bk2k2</td>\n",
       "      <td>Brother Bear 2</td>\n",
       "      <td>2006-08-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>{\"/m/02h40lc\": \"English Language\"}</td>\n",
       "      <td>{\"/m/09c7w0\": \"United States of America\"}</td>\n",
       "      <td>{\"/m/0hqxf\": \"Family Film\", \"/m/0hcr\": \"Animat...</td>\n",
       "      <td>brother bear 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17561</th>\n",
       "      <td>131260</td>\n",
       "      <td>Rentun Ruusu (2001)</td>\n",
       "      <td>(no genres listed)</td>\n",
       "      <td>rentun ruusu</td>\n",
       "      <td>10166655</td>\n",
       "      <td>/m/02q3_cx</td>\n",
       "      <td>Rentun Ruusu</td>\n",
       "      <td>2001-01-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>{\"/m/01gp_d\": \"Finnish Language\"}</td>\n",
       "      <td>{\"/m/02vzc\": \"Finland\"}</td>\n",
       "      <td>{\"/m/03bxz7\": \"Biographical film\"}</td>\n",
       "      <td>rentun ruusu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17562 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                               title  \\\n",
       "0            1                    Toy Story (1995)   \n",
       "1            2                      Jumanji (1995)   \n",
       "2            3             Grumpier Old Men (1995)   \n",
       "3            4            Waiting to Exhale (1995)   \n",
       "4            5  Father of the Bride Part II (1995)   \n",
       "...        ...                                 ...   \n",
       "17557   131152                  The Fat Spy (1966)   \n",
       "17558   131166                   WWII IN HD (2009)   \n",
       "17559   131239           Three Quarter Moon (2011)   \n",
       "17560   131248               Brother Bear 2 (2006)   \n",
       "17561   131260                 Rentun Ruusu (2001)   \n",
       "\n",
       "                                          genres_x  \\\n",
       "0      Adventure|Animation|Children|Comedy|Fantasy   \n",
       "1                       Adventure|Children|Fantasy   \n",
       "2                                   Comedy|Romance   \n",
       "3                             Comedy|Drama|Romance   \n",
       "4                                           Comedy   \n",
       "...                                            ...   \n",
       "17557                                       Comedy   \n",
       "17558                           (no genres listed)   \n",
       "17559                                 Comedy|Drama   \n",
       "17560  Adventure|Animation|Children|Comedy|Fantasy   \n",
       "17561                           (no genres listed)   \n",
       "\n",
       "                      title_format  wikipedia_movie_id freebase_movie_id  \\\n",
       "0                        toy story               53085          /m/0dyb1   \n",
       "1                          jumanji             3700174         /m/09w353   \n",
       "2                 grumpier old men             1934035         /m/0676dr   \n",
       "3                waiting to exhale              972970         /m/03vny7   \n",
       "4      father of the bride part ii             3303622         /m/094g2z   \n",
       "...                            ...                 ...               ...   \n",
       "17557                  the fat spy             1853200         /m/0613sx   \n",
       "17558                   wwii in hd            26321497        /m/0bbxzy2   \n",
       "17559           three quarter moon            33597212        /m/0hgrd5g   \n",
       "17560               brother bear 2             4118248         /m/0bk2k2   \n",
       "17561                 rentun ruusu            10166655        /m/02q3_cx   \n",
       "\n",
       "                        movie_name release_date  box_office_revenue  runtime  \\\n",
       "0                        Toy Story   1995-11-19         361958736.0     77.0   \n",
       "1                          Jumanji   1995-12-15         262797249.0    104.0   \n",
       "2                 Grumpier Old Men   1995-12-22          71518503.0    101.0   \n",
       "3                Waiting to Exhale   1995-12-22          81452156.0    121.0   \n",
       "4      Father of the Bride Part II   1995-12-08          76594107.0    106.0   \n",
       "...                            ...          ...                 ...      ...   \n",
       "17557                  The Fat Spy         1966                 NaN     75.0   \n",
       "17558                   WWII in HD          NaN                 NaN      NaN   \n",
       "17559           Three Quarter Moon   2011-09-30                 NaN     95.0   \n",
       "17560               Brother Bear 2   2006-08-29                 NaN     73.0   \n",
       "17561                 Rentun Ruusu   2001-01-12                 NaN    100.0   \n",
       "\n",
       "                                               languages  \\\n",
       "0                     {\"/m/02h40lc\": \"English Language\"}   \n",
       "1      {\"/m/064_8sq\": \"French Language\", \"/m/02h40lc\"...   \n",
       "2      {\"/m/02bjrlw\": \"Italian Language\", \"/m/02h40lc...   \n",
       "3                     {\"/m/02h40lc\": \"English Language\"}   \n",
       "4                     {\"/m/02h40lc\": \"English Language\"}   \n",
       "...                                                  ...   \n",
       "17557                 {\"/m/02h40lc\": \"English Language\"}   \n",
       "17558                                                 {}   \n",
       "17559  {\"/m/02hwyss\": \"Turkish Language\", \"/m/0gtg\": ...   \n",
       "17560                 {\"/m/02h40lc\": \"English Language\"}   \n",
       "17561                  {\"/m/01gp_d\": \"Finnish Language\"}   \n",
       "\n",
       "                                               countries  \\\n",
       "0              {\"/m/09c7w0\": \"United States of America\"}   \n",
       "1      {\"/m/09c7w0\": \"United States of America\", \"/m/...   \n",
       "2              {\"/m/09c7w0\": \"United States of America\"}   \n",
       "3              {\"/m/09c7w0\": \"United States of America\"}   \n",
       "4              {\"/m/09c7w0\": \"United States of America\"}   \n",
       "...                                                  ...   \n",
       "17557          {\"/m/09c7w0\": \"United States of America\"}   \n",
       "17558                                                 {}   \n",
       "17559                            {\"/m/0345h\": \"Germany\"}   \n",
       "17560          {\"/m/09c7w0\": \"United States of America\"}   \n",
       "17561                            {\"/m/02vzc\": \"Finland\"}   \n",
       "\n",
       "                                                genres_y  \\\n",
       "0      {\"/m/0556j8\": \"Buddy film\", \"/m/03k9fj\": \"Adve...   \n",
       "1      {\"/m/01jfsb\": \"Thriller\", \"/m/0hj3n2k\": \"Fanta...   \n",
       "2      {\"/m/06cvj\": \"Romantic comedy\", \"/m/0556j8\": \"...   \n",
       "3      {\"/m/0hj3n0w\": \"Ensemble Film\", \"/m/06w2n3t\": ...   \n",
       "4      {\"/m/06qm3\": \"Screwball comedy\", \"/m/02l7c8\": ...   \n",
       "...                                                  ...   \n",
       "17557  {\"/m/0gf28\": \"Parody\", \"/m/0220p9g\": \"Musical ...   \n",
       "17558                           {\"/m/082gq\": \"War film\"}   \n",
       "17559  {\"/m/0556j8\": \"Buddy film\", \"/m/0hqxf\": \"Famil...   \n",
       "17560  {\"/m/0hqxf\": \"Family Film\", \"/m/0hcr\": \"Animat...   \n",
       "17561                 {\"/m/03bxz7\": \"Biographical film\"}   \n",
       "\n",
       "              movie_name_formatted  \n",
       "0                        toy story  \n",
       "1                          jumanji  \n",
       "2                 grumpier old men  \n",
       "3                waiting to exhale  \n",
       "4      father of the bride part ii  \n",
       "...                            ...  \n",
       "17557                  the fat spy  \n",
       "17558                   wwii in hd  \n",
       "17559           three quarter moon  \n",
       "17560               brother bear 2  \n",
       "17561                 rentun ruusu  \n",
       "\n",
       "[17562 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this user from the dataset, because it has too many ratings\n",
    "df = df[df['userId'] != 175325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_genres=movies['genres'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User with the most ratings:\n",
      "118205    9254\n",
      "8405      7515\n",
      "82418     5646\n",
      "121535    5520\n",
      "125794    5491\n",
      "74142     5447\n",
      "34576     5356\n",
      "131904    5330\n",
      "83090     5169\n",
      "59477     4988\n",
      "130767    4785\n",
      "79159     4707\n",
      "8963      4524\n",
      "15617     4354\n",
      "92011     4236\n",
      "71975     4182\n",
      "20132     4101\n",
      "46470     4094\n",
      "88820     4093\n",
      "63147     3958\n",
      "Name: userId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the user with the most ratings\n",
    "user_ratings_count = df['userId'].value_counts()\n",
    "\n",
    "print('User with the most ratings:')\n",
    "print(user_ratings_count.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_films=pd.merge(movies,df,on='movieId',how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMAX', 'Crime', 'Mystery', 'Sci-Fi', 'Musical', 'Comedy', 'Children', 'Adventure', 'Film-Noir', 'Thriller', 'Animation', 'Action', 'Western', 'Documentary', 'War', 'Horror', 'Drama', 'Fantasy', 'Romance']\n"
     ]
    }
   ],
   "source": [
    "unique_genres = movies['genres'].unique()\n",
    "all_genres = set('|'.join(unique_genres).split('|'))\n",
    "\n",
    "all_genres_list = list(all_genres)\n",
    "all_genres_list.remove('(no genres listed)')\n",
    "print(all_genres_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>title_format</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>IMAX</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>...</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Action</th>\n",
       "      <th>Western</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>War</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Romance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1999-12-11 13:36:47</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1997-03-13 17:50:52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1996-06-05 13:37:51</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1999-11-25 02:44:47</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>toy story</td>\n",
       "      <td>11</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2009-01-02 01:13:41</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000258</th>\n",
       "      <td>131254</td>\n",
       "      <td>Kein Bund für's Leben (2007)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>kein bund für's leben</td>\n",
       "      <td>79570</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015-03-30 19:32:59</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000259</th>\n",
       "      <td>131256</td>\n",
       "      <td>Feuer, Eis &amp; Dosenbier (2002)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>feuer, eis &amp; dosenbier</td>\n",
       "      <td>79570</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015-03-30 19:48:08</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000260</th>\n",
       "      <td>131258</td>\n",
       "      <td>The Pirates (2014)</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>the pirates</td>\n",
       "      <td>28906</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2015-03-30 19:56:32</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000261</th>\n",
       "      <td>131260</td>\n",
       "      <td>Rentun Ruusu (2001)</td>\n",
       "      <td>(no genres listed)</td>\n",
       "      <td>rentun ruusu</td>\n",
       "      <td>65409</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015-03-30 19:57:46</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000262</th>\n",
       "      <td>131262</td>\n",
       "      <td>Innocence (2014)</td>\n",
       "      <td>Adventure|Fantasy|Horror</td>\n",
       "      <td>innocence</td>\n",
       "      <td>133047</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015-03-30 20:39:26</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000263 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          movieId                          title  \\\n",
       "0               1               Toy Story (1995)   \n",
       "1               1               Toy Story (1995)   \n",
       "2               1               Toy Story (1995)   \n",
       "3               1               Toy Story (1995)   \n",
       "4               1               Toy Story (1995)   \n",
       "...           ...                            ...   \n",
       "20000258   131254   Kein Bund für's Leben (2007)   \n",
       "20000259   131256  Feuer, Eis & Dosenbier (2002)   \n",
       "20000260   131258             The Pirates (2014)   \n",
       "20000261   131260            Rentun Ruusu (2001)   \n",
       "20000262   131262               Innocence (2014)   \n",
       "\n",
       "                                               genres            title_format  \\\n",
       "0         Adventure|Animation|Children|Comedy|Fantasy               toy story   \n",
       "1         Adventure|Animation|Children|Comedy|Fantasy               toy story   \n",
       "2         Adventure|Animation|Children|Comedy|Fantasy               toy story   \n",
       "3         Adventure|Animation|Children|Comedy|Fantasy               toy story   \n",
       "4         Adventure|Animation|Children|Comedy|Fantasy               toy story   \n",
       "...                                               ...                     ...   \n",
       "20000258                                       Comedy   kein bund für's leben   \n",
       "20000259                                       Comedy  feuer, eis & dosenbier   \n",
       "20000260                                    Adventure             the pirates   \n",
       "20000261                           (no genres listed)            rentun ruusu   \n",
       "20000262                     Adventure|Fantasy|Horror               innocence   \n",
       "\n",
       "          userId  rating            timestamp   IMAX  Crime  Mystery  ...  \\\n",
       "0              3     4.0  1999-12-11 13:36:47  False  False    False  ...   \n",
       "1              6     5.0  1997-03-13 17:50:52  False  False    False  ...   \n",
       "2              8     4.0  1996-06-05 13:37:51  False  False    False  ...   \n",
       "3             10     4.0  1999-11-25 02:44:47  False  False    False  ...   \n",
       "4             11     4.5  2009-01-02 01:13:41  False  False    False  ...   \n",
       "...          ...     ...                  ...    ...    ...      ...  ...   \n",
       "20000258   79570     4.0  2015-03-30 19:32:59  False  False    False  ...   \n",
       "20000259   79570     4.0  2015-03-30 19:48:08  False  False    False  ...   \n",
       "20000260   28906     2.5  2015-03-30 19:56:32  False  False    False  ...   \n",
       "20000261   65409     3.0  2015-03-30 19:57:46  False  False    False  ...   \n",
       "20000262  133047     4.0  2015-03-30 20:39:26  False  False    False  ...   \n",
       "\n",
       "          Thriller  Animation  Action  Western  Documentary    War  Horror  \\\n",
       "0            False       True   False    False        False  False   False   \n",
       "1            False       True   False    False        False  False   False   \n",
       "2            False       True   False    False        False  False   False   \n",
       "3            False       True   False    False        False  False   False   \n",
       "4            False       True   False    False        False  False   False   \n",
       "...            ...        ...     ...      ...          ...    ...     ...   \n",
       "20000258     False      False   False    False        False  False   False   \n",
       "20000259     False      False   False    False        False  False   False   \n",
       "20000260     False      False   False    False        False  False   False   \n",
       "20000261     False      False   False    False        False  False   False   \n",
       "20000262     False      False   False    False        False  False    True   \n",
       "\n",
       "          Drama  Fantasy  Romance  \n",
       "0         False     True    False  \n",
       "1         False     True    False  \n",
       "2         False     True    False  \n",
       "3         False     True    False  \n",
       "4         False     True    False  \n",
       "...         ...      ...      ...  \n",
       "20000258  False    False    False  \n",
       "20000259  False    False    False  \n",
       "20000260  False    False    False  \n",
       "20000261  False    False    False  \n",
       "20000262  False     True    False  \n",
       "\n",
       "[20000263 rows x 26 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a one hot encoding for each genre\n",
    "for genre in all_genres_list : \n",
    "    rating_films[genre] = rating_films['genres'].apply(lambda x: genre in x.split('|'))\n",
    "rating_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_for_userID_generator(userId):\n",
    "    user_rating = rating_films[rating_films['userId'] == userId]\n",
    "    user_rating = user_rating.iloc[:, 5:].sample(frac=1)  \n",
    "    user_rating = user_rating.drop(columns=['timestamp'])\n",
    "    y = user_rating['rating']\n",
    "    x = user_rating.drop(columns=['rating'])\n",
    "    \n",
    "    length = len(y)\n",
    "    x_size = int(length * 0.8) \n",
    "    \n",
    "    x_train = x.iloc[:x_size, :]\n",
    "    x_test = x.iloc[x_size:, :]\n",
    "    y_train = y.iloc[:x_size]\n",
    "    y_test = y.iloc[x_size:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = rating_for_userID_generator(8405)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "151/151 [==============================] - 2s 4ms/step - loss: 2.2966 - mse: 2.2966 - val_loss: 0.8898 - val_mse: 0.8898\n",
      "Epoch 2/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.8343 - mse: 0.8343 - val_loss: 0.8070 - val_mse: 0.8070\n",
      "Epoch 3/100\n",
      "151/151 [==============================] - 0s 3ms/step - loss: 0.7923 - mse: 0.7923 - val_loss: 0.7792 - val_mse: 0.7792\n",
      "Epoch 4/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7807 - mse: 0.7807 - val_loss: 0.7674 - val_mse: 0.7674\n",
      "Epoch 5/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7691 - mse: 0.7691 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 6/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7654 - mse: 0.7654 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 7/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.7606 - val_loss: 0.7863 - val_mse: 0.7863\n",
      "Epoch 8/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7610 - mse: 0.7610 - val_loss: 0.7563 - val_mse: 0.7563\n",
      "Epoch 9/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.7559 - val_loss: 0.7571 - val_mse: 0.7571\n",
      "Epoch 10/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7578 - mse: 0.7578 - val_loss: 0.7500 - val_mse: 0.7500\n",
      "Epoch 11/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7495 - mse: 0.7495 - val_loss: 0.7524 - val_mse: 0.7524\n",
      "Epoch 12/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7508 - mse: 0.7508 - val_loss: 0.7500 - val_mse: 0.7500\n",
      "Epoch 13/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7481 - mse: 0.7481 - val_loss: 0.7492 - val_mse: 0.7492\n",
      "Epoch 14/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7457 - mse: 0.7457 - val_loss: 0.7655 - val_mse: 0.7655\n",
      "Epoch 15/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7453 - mse: 0.7453 - val_loss: 0.7516 - val_mse: 0.7516\n",
      "Epoch 16/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7442 - mse: 0.7442 - val_loss: 0.7515 - val_mse: 0.7515\n",
      "Epoch 17/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7404 - mse: 0.7404 - val_loss: 0.7570 - val_mse: 0.7570\n",
      "Epoch 18/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7411 - mse: 0.7411 - val_loss: 0.7617 - val_mse: 0.7617\n",
      "Epoch 19/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7368 - mse: 0.7368 - val_loss: 0.7523 - val_mse: 0.7523\n",
      "Epoch 20/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7393 - mse: 0.7393 - val_loss: 0.7654 - val_mse: 0.7654\n",
      "Epoch 21/100\n",
      "151/151 [==============================] - 0s 3ms/step - loss: 0.7369 - mse: 0.7369 - val_loss: 0.7596 - val_mse: 0.7596\n",
      "Epoch 22/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7374 - mse: 0.7374 - val_loss: 0.7563 - val_mse: 0.7563\n",
      "Epoch 23/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7369 - mse: 0.7369 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 24/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7338 - mse: 0.7338 - val_loss: 0.7618 - val_mse: 0.7618\n",
      "Epoch 25/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7318 - mse: 0.7318 - val_loss: 0.7723 - val_mse: 0.7723\n",
      "Epoch 26/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7338 - mse: 0.7338 - val_loss: 0.7756 - val_mse: 0.7756\n",
      "Epoch 27/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7312 - mse: 0.7312 - val_loss: 0.7665 - val_mse: 0.7665\n",
      "Epoch 28/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7294 - mse: 0.7294 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 29/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7301 - mse: 0.7301 - val_loss: 0.7843 - val_mse: 0.7843\n",
      "Epoch 30/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7323 - mse: 0.7323 - val_loss: 0.7762 - val_mse: 0.7762\n",
      "Epoch 31/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7268 - mse: 0.7268 - val_loss: 0.7616 - val_mse: 0.7616\n",
      "Epoch 32/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7278 - mse: 0.7278 - val_loss: 0.7805 - val_mse: 0.7805\n",
      "Epoch 33/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7259 - mse: 0.7259 - val_loss: 0.7647 - val_mse: 0.7647\n",
      "Epoch 34/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7260 - mse: 0.7260 - val_loss: 0.7720 - val_mse: 0.7720\n",
      "Epoch 35/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7235 - mse: 0.7235 - val_loss: 0.7627 - val_mse: 0.7627\n",
      "Epoch 36/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7221 - mse: 0.7221 - val_loss: 0.7641 - val_mse: 0.7641\n",
      "Epoch 37/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7235 - mse: 0.7235 - val_loss: 0.7728 - val_mse: 0.7728\n",
      "Epoch 38/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 39/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7194 - mse: 0.7194 - val_loss: 0.7727 - val_mse: 0.7727\n",
      "Epoch 40/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7213 - mse: 0.7213 - val_loss: 0.7760 - val_mse: 0.7760\n",
      "Epoch 41/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7213 - mse: 0.7213 - val_loss: 0.7718 - val_mse: 0.7718\n",
      "Epoch 42/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7233 - mse: 0.7233 - val_loss: 0.7828 - val_mse: 0.7828\n",
      "Epoch 43/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7191 - mse: 0.7191 - val_loss: 0.7771 - val_mse: 0.7771\n",
      "Epoch 44/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7200 - mse: 0.7200 - val_loss: 0.7751 - val_mse: 0.7751\n",
      "Epoch 45/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7173 - mse: 0.7173 - val_loss: 0.7691 - val_mse: 0.7691\n",
      "Epoch 46/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7168 - mse: 0.7168 - val_loss: 0.7738 - val_mse: 0.7738\n",
      "Epoch 47/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7127 - mse: 0.7127 - val_loss: 0.7745 - val_mse: 0.7745\n",
      "Epoch 48/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7137 - mse: 0.7137 - val_loss: 0.7723 - val_mse: 0.7723\n",
      "Epoch 49/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7158 - mse: 0.7158 - val_loss: 0.7996 - val_mse: 0.7996\n",
      "Epoch 50/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7149 - mse: 0.7149 - val_loss: 0.7717 - val_mse: 0.7717\n",
      "Epoch 51/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7177 - mse: 0.7177 - val_loss: 0.7810 - val_mse: 0.7810\n",
      "Epoch 52/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7182 - mse: 0.7182 - val_loss: 0.7777 - val_mse: 0.7777\n",
      "Epoch 53/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7131 - mse: 0.7131 - val_loss: 0.7929 - val_mse: 0.7929\n",
      "Epoch 54/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7122 - mse: 0.7122 - val_loss: 0.7793 - val_mse: 0.7793\n",
      "Epoch 55/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7131 - mse: 0.7131 - val_loss: 0.7997 - val_mse: 0.7997\n",
      "Epoch 56/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7148 - mse: 0.7148 - val_loss: 0.7734 - val_mse: 0.7734\n",
      "Epoch 57/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7117 - mse: 0.7117 - val_loss: 0.7788 - val_mse: 0.7788\n",
      "Epoch 58/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7112 - mse: 0.7112 - val_loss: 0.7754 - val_mse: 0.7754\n",
      "Epoch 59/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7863 - val_mse: 0.7863\n",
      "Epoch 60/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7094 - mse: 0.7094 - val_loss: 0.8279 - val_mse: 0.8279\n",
      "Epoch 61/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7068 - mse: 0.7068 - val_loss: 0.7859 - val_mse: 0.7859\n",
      "Epoch 62/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7119 - mse: 0.7119 - val_loss: 0.7889 - val_mse: 0.7889\n",
      "Epoch 63/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7075 - mse: 0.7075 - val_loss: 0.7831 - val_mse: 0.7831\n",
      "Epoch 64/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7066 - mse: 0.7066 - val_loss: 0.7847 - val_mse: 0.7847\n",
      "Epoch 65/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7088 - mse: 0.7088 - val_loss: 0.7878 - val_mse: 0.7878\n",
      "Epoch 66/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7065 - mse: 0.7065 - val_loss: 0.7852 - val_mse: 0.7852\n",
      "Epoch 67/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7103 - mse: 0.7103 - val_loss: 0.7874 - val_mse: 0.7874\n",
      "Epoch 68/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7092 - mse: 0.7092 - val_loss: 0.7896 - val_mse: 0.7896\n",
      "Epoch 69/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7054 - mse: 0.7054 - val_loss: 0.7867 - val_mse: 0.7867\n",
      "Epoch 70/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7065 - mse: 0.7065 - val_loss: 0.7887 - val_mse: 0.7887\n",
      "Epoch 71/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7024 - mse: 0.7024 - val_loss: 0.7867 - val_mse: 0.7867\n",
      "Epoch 72/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7048 - mse: 0.7048 - val_loss: 0.8009 - val_mse: 0.8009\n",
      "Epoch 73/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7079 - mse: 0.7079 - val_loss: 0.7811 - val_mse: 0.7811\n",
      "Epoch 74/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7042 - mse: 0.7042 - val_loss: 0.7995 - val_mse: 0.7995\n",
      "Epoch 75/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7059 - mse: 0.7059 - val_loss: 0.7902 - val_mse: 0.7902\n",
      "Epoch 76/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7015 - mse: 0.7015 - val_loss: 0.7971 - val_mse: 0.7971\n",
      "Epoch 77/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7013 - mse: 0.7013 - val_loss: 0.7899 - val_mse: 0.7899\n",
      "Epoch 78/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7038 - mse: 0.7038 - val_loss: 0.7918 - val_mse: 0.7918\n",
      "Epoch 79/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7054 - mse: 0.7054 - val_loss: 0.7949 - val_mse: 0.7949\n",
      "Epoch 80/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7024 - mse: 0.7024 - val_loss: 0.7957 - val_mse: 0.7957\n",
      "Epoch 81/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7006 - mse: 0.7006 - val_loss: 0.7961 - val_mse: 0.7961\n",
      "Epoch 82/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6998 - mse: 0.6998 - val_loss: 0.7890 - val_mse: 0.7890\n",
      "Epoch 83/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6998 - mse: 0.6998 - val_loss: 0.7942 - val_mse: 0.7942\n",
      "Epoch 84/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7024 - mse: 0.7024 - val_loss: 0.7972 - val_mse: 0.7972\n",
      "Epoch 85/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7020 - mse: 0.7020 - val_loss: 0.8107 - val_mse: 0.8107\n",
      "Epoch 86/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7006 - mse: 0.7006 - val_loss: 0.7986 - val_mse: 0.7986\n",
      "Epoch 87/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6992 - mse: 0.6992 - val_loss: 0.8021 - val_mse: 0.8021\n",
      "Epoch 88/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6978 - mse: 0.6978 - val_loss: 0.7965 - val_mse: 0.7965\n",
      "Epoch 89/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7038 - mse: 0.7038 - val_loss: 0.7981 - val_mse: 0.7981\n",
      "Epoch 90/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6961 - mse: 0.6961 - val_loss: 0.7948 - val_mse: 0.7948\n",
      "Epoch 91/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6956 - mse: 0.6956 - val_loss: 0.8016 - val_mse: 0.8016\n",
      "Epoch 92/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6965 - mse: 0.6965 - val_loss: 0.7955 - val_mse: 0.7955\n",
      "Epoch 93/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6987 - mse: 0.6987 - val_loss: 0.8058 - val_mse: 0.8058\n",
      "Epoch 94/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6954 - mse: 0.6954 - val_loss: 0.8088 - val_mse: 0.8088\n",
      "Epoch 95/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6954 - mse: 0.6954 - val_loss: 0.7996 - val_mse: 0.7996\n",
      "Epoch 96/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6951 - mse: 0.6951 - val_loss: 0.8068 - val_mse: 0.8068\n",
      "Epoch 97/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6971 - mse: 0.6971 - val_loss: 0.8015 - val_mse: 0.8015\n",
      "Epoch 98/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.7958 - val_mse: 0.7958\n",
      "Epoch 99/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.7996 - val_mse: 0.7996\n",
      "Epoch 100/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6972 - mse: 0.6972 - val_loss: 0.8124 - val_mse: 0.8124\n",
      "188/188 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "Training Set Evaluation:\n",
      "MSE: 0.713561445695775\n",
      "R^2 Score: 0.20838713195924097\n",
      "\n",
      "Test Set Evaluation:\n",
      "MSE: 0.8268195316257041\n",
      "R^2 Score: 0.07564023800537356\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=x_train.shape[1], activation='relu'),  # First hidden layer\n",
    "    Dense(32, activation='relu'),  # Second hidden layer\n",
    "    Dense(1, activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predict on both training and test sets\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"MSE:\", train_mse)\n",
    "print(\"R^2 Score:\", train_r2)\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(\"MSE:\", test_mse)\n",
    "print(\"R^2 Score:\", test_r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "151/151 [==============================] - 1s 3ms/step - loss: 2.7443 - mse: 2.7443 - val_loss: 0.8438 - val_mse: 0.8438\n",
      "Epoch 2/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.8279 - mse: 0.8279 - val_loss: 0.7574 - val_mse: 0.7574\n",
      "Epoch 3/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7862 - mse: 0.7862 - val_loss: 0.7422 - val_mse: 0.7422\n",
      "Epoch 4/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7678 - mse: 0.7678 - val_loss: 0.7473 - val_mse: 0.7473\n",
      "Epoch 5/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7630 - mse: 0.7630 - val_loss: 0.7568 - val_mse: 0.7568\n",
      "Epoch 6/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7605 - mse: 0.7605 - val_loss: 0.7484 - val_mse: 0.7484\n",
      "Epoch 7/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7554 - mse: 0.7554 - val_loss: 0.7423 - val_mse: 0.7423\n",
      "Epoch 8/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7498 - mse: 0.7498 - val_loss: 0.7439 - val_mse: 0.7439\n",
      "Epoch 9/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7475 - mse: 0.7475 - val_loss: 0.7482 - val_mse: 0.7482\n",
      "Epoch 10/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7446 - mse: 0.7446 - val_loss: 0.7443 - val_mse: 0.7443\n",
      "Epoch 11/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7480 - mse: 0.7480 - val_loss: 0.7540 - val_mse: 0.7540\n",
      "Epoch 12/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7387 - mse: 0.7387 - val_loss: 0.7468 - val_mse: 0.7468\n",
      "Epoch 13/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7418 - mse: 0.7418 - val_loss: 0.7529 - val_mse: 0.7529\n",
      "Epoch 14/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7391 - mse: 0.7391 - val_loss: 0.7593 - val_mse: 0.7593\n",
      "Epoch 15/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7348 - mse: 0.7348 - val_loss: 0.7491 - val_mse: 0.7491\n",
      "Epoch 16/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7345 - mse: 0.7345 - val_loss: 0.7630 - val_mse: 0.7630\n",
      "Epoch 17/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7360 - mse: 0.7360 - val_loss: 0.7595 - val_mse: 0.7595\n",
      "Epoch 18/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7358 - mse: 0.7358 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "Epoch 19/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7306 - mse: 0.7306 - val_loss: 0.7601 - val_mse: 0.7601\n",
      "Epoch 20/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7310 - mse: 0.7310 - val_loss: 0.7499 - val_mse: 0.7499\n",
      "Epoch 21/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7308 - mse: 0.7308 - val_loss: 0.7485 - val_mse: 0.7485\n",
      "Epoch 22/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7254 - mse: 0.7254 - val_loss: 0.7451 - val_mse: 0.7451\n",
      "Epoch 23/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7277 - mse: 0.7277 - val_loss: 0.7475 - val_mse: 0.7475\n",
      "Epoch 24/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7256 - mse: 0.7256 - val_loss: 0.7514 - val_mse: 0.7514\n",
      "Epoch 25/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7216 - mse: 0.7216 - val_loss: 0.7579 - val_mse: 0.7579\n",
      "Epoch 26/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7242 - mse: 0.7242 - val_loss: 0.7644 - val_mse: 0.7644\n",
      "Epoch 27/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7255 - mse: 0.7255 - val_loss: 0.7545 - val_mse: 0.7545\n",
      "Epoch 28/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7280 - mse: 0.7280 - val_loss: 0.7652 - val_mse: 0.7652\n",
      "Epoch 29/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7653 - val_mse: 0.7653\n",
      "Epoch 30/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.7520 - val_mse: 0.7520\n",
      "Epoch 31/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7207 - mse: 0.7207 - val_loss: 0.7834 - val_mse: 0.7834\n",
      "Epoch 32/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7616 - val_mse: 0.7616\n",
      "Epoch 33/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7193 - mse: 0.7193 - val_loss: 0.7664 - val_mse: 0.7664\n",
      "Epoch 34/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7203 - mse: 0.7203 - val_loss: 0.7555 - val_mse: 0.7555\n",
      "Epoch 35/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7683 - val_mse: 0.7683\n",
      "Epoch 36/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7186 - mse: 0.7186 - val_loss: 0.7548 - val_mse: 0.7548\n",
      "Epoch 37/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7198 - mse: 0.7198 - val_loss: 0.7957 - val_mse: 0.7957\n",
      "Epoch 38/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7192 - mse: 0.7192 - val_loss: 0.7597 - val_mse: 0.7597\n",
      "Epoch 39/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7204 - mse: 0.7204 - val_loss: 0.7572 - val_mse: 0.7572\n",
      "Epoch 40/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7167 - mse: 0.7167 - val_loss: 0.7712 - val_mse: 0.7712\n",
      "Epoch 41/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7172 - mse: 0.7172 - val_loss: 0.7796 - val_mse: 0.7796\n",
      "Epoch 42/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7141 - mse: 0.7141 - val_loss: 0.7824 - val_mse: 0.7824\n",
      "Epoch 43/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7130 - mse: 0.7130 - val_loss: 0.7584 - val_mse: 0.7584\n",
      "Epoch 44/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7112 - mse: 0.7112 - val_loss: 0.7648 - val_mse: 0.7648\n",
      "Epoch 45/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7124 - mse: 0.7124 - val_loss: 0.7791 - val_mse: 0.7791\n",
      "Epoch 46/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7166 - mse: 0.7166 - val_loss: 0.7579 - val_mse: 0.7579\n",
      "Epoch 47/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7116 - mse: 0.7116 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "Epoch 48/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7076 - mse: 0.7076 - val_loss: 0.7659 - val_mse: 0.7659\n",
      "Epoch 49/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7079 - mse: 0.7079 - val_loss: 0.7696 - val_mse: 0.7696\n",
      "Epoch 50/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7088 - mse: 0.7088 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 51/100\n",
      "151/151 [==============================] - 0s 3ms/step - loss: 0.7101 - mse: 0.7101 - val_loss: 0.7636 - val_mse: 0.7636\n",
      "Epoch 52/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7105 - mse: 0.7105 - val_loss: 0.7647 - val_mse: 0.7647\n",
      "Epoch 53/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7102 - mse: 0.7102 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 54/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7082 - mse: 0.7082 - val_loss: 0.7743 - val_mse: 0.7743\n",
      "Epoch 55/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7052 - mse: 0.7052 - val_loss: 0.7657 - val_mse: 0.7657\n",
      "Epoch 56/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7056 - mse: 0.7056 - val_loss: 0.7733 - val_mse: 0.7733\n",
      "Epoch 57/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7055 - mse: 0.7055 - val_loss: 0.7740 - val_mse: 0.7740\n",
      "Epoch 58/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7069 - mse: 0.7069 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 59/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7038 - mse: 0.7038 - val_loss: 0.7725 - val_mse: 0.7725\n",
      "Epoch 60/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7075 - mse: 0.7075 - val_loss: 0.7737 - val_mse: 0.7737\n",
      "Epoch 61/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7031 - mse: 0.7031 - val_loss: 0.7699 - val_mse: 0.7699\n",
      "Epoch 62/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7068 - mse: 0.7068 - val_loss: 0.7907 - val_mse: 0.7907\n",
      "Epoch 63/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7025 - mse: 0.7025 - val_loss: 0.7735 - val_mse: 0.7735\n",
      "Epoch 64/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7033 - mse: 0.7033 - val_loss: 0.7789 - val_mse: 0.7789\n",
      "Epoch 65/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7030 - mse: 0.7030 - val_loss: 0.7761 - val_mse: 0.7761\n",
      "Epoch 66/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7003 - mse: 0.7003 - val_loss: 0.7815 - val_mse: 0.7815\n",
      "Epoch 67/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7041 - mse: 0.7041 - val_loss: 0.7797 - val_mse: 0.7797\n",
      "Epoch 68/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6994 - mse: 0.6994 - val_loss: 0.7884 - val_mse: 0.7884\n",
      "Epoch 69/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7015 - mse: 0.7015 - val_loss: 0.7816 - val_mse: 0.7816\n",
      "Epoch 70/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7018 - mse: 0.7018 - val_loss: 0.7892 - val_mse: 0.7892\n",
      "Epoch 71/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7046 - mse: 0.7046 - val_loss: 0.7709 - val_mse: 0.7709\n",
      "Epoch 72/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7041 - mse: 0.7041 - val_loss: 0.7668 - val_mse: 0.7668\n",
      "Epoch 73/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7809 - val_mse: 0.7809\n",
      "Epoch 74/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7007 - mse: 0.7007 - val_loss: 0.7764 - val_mse: 0.7764\n",
      "Epoch 75/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6979 - mse: 0.6979 - val_loss: 0.7749 - val_mse: 0.7749\n",
      "Epoch 76/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7029 - mse: 0.7029 - val_loss: 0.7821 - val_mse: 0.7821\n",
      "Epoch 77/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6993 - mse: 0.6993 - val_loss: 0.7662 - val_mse: 0.7662\n",
      "Epoch 78/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6969 - mse: 0.6969 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 79/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6991 - mse: 0.6991 - val_loss: 0.7845 - val_mse: 0.7845\n",
      "Epoch 80/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6975 - mse: 0.6975 - val_loss: 0.7784 - val_mse: 0.7784\n",
      "Epoch 81/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6970 - mse: 0.6970 - val_loss: 0.7785 - val_mse: 0.7785\n",
      "Epoch 82/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6998 - mse: 0.6998 - val_loss: 0.7823 - val_mse: 0.7823\n",
      "Epoch 83/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6947 - mse: 0.6947 - val_loss: 0.7852 - val_mse: 0.7852\n",
      "Epoch 84/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.7000 - mse: 0.7000 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 85/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6941 - mse: 0.6941 - val_loss: 0.7824 - val_mse: 0.7824\n",
      "Epoch 86/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6989 - mse: 0.6989 - val_loss: 0.7740 - val_mse: 0.7740\n",
      "Epoch 87/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6968 - mse: 0.6968 - val_loss: 0.7750 - val_mse: 0.7750\n",
      "Epoch 88/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6953 - mse: 0.6953 - val_loss: 0.7781 - val_mse: 0.7781\n",
      "Epoch 89/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6922 - mse: 0.6922 - val_loss: 0.7682 - val_mse: 0.7682\n",
      "Epoch 90/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6953 - mse: 0.6953 - val_loss: 0.7811 - val_mse: 0.7811\n",
      "Epoch 91/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6971 - mse: 0.6971 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 92/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6925 - mse: 0.6925 - val_loss: 0.7848 - val_mse: 0.7848\n",
      "Epoch 93/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6924 - mse: 0.6924 - val_loss: 0.7922 - val_mse: 0.7922\n",
      "Epoch 94/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6929 - mse: 0.6929 - val_loss: 0.7756 - val_mse: 0.7756\n",
      "Epoch 95/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6969 - mse: 0.6969 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 96/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6963 - mse: 0.6963 - val_loss: 0.7743 - val_mse: 0.7743\n",
      "Epoch 97/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6984 - mse: 0.6984 - val_loss: 0.7727 - val_mse: 0.7727\n",
      "Epoch 98/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6970 - mse: 0.6970 - val_loss: 0.7964 - val_mse: 0.7964\n",
      "Epoch 99/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6910 - mse: 0.6910 - val_loss: 0.7804 - val_mse: 0.7804\n",
      "Epoch 100/100\n",
      "151/151 [==============================] - 0s 2ms/step - loss: 0.6947 - mse: 0.6947 - val_loss: 0.7730 - val_mse: 0.7730\n",
      "188/188 [==============================] - 0s 955us/step\n",
      "47/47 [==============================] - 0s 940us/step\n",
      "Evaluation for User ID 8405:\n",
      "Training Set Evaluation:\n",
      "MSE: 0.6977, R^2: 0.2174\n",
      "\n",
      "Test Set Evaluation:\n",
      "MSE: 0.8326, R^2: 0.1083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_model_for_user(userId, data):\n",
    "    \"\"\"\n",
    "    Generates training and test splits for a given user ID, trains a neural network, \n",
    "    and evaluates the model to return MSE and R^2 for both training and test sets.\n",
    "    \"\"\"\n",
    "    # Filter and preprocess data for the given user\n",
    "    user_rating = data[data['userId'] == userId]\n",
    "    user_rating = user_rating.iloc[:, 5:].sample(frac=1)  # Shuffle data\n",
    "    user_rating = user_rating.drop(columns=['timestamp'])\n",
    "    y = user_rating['rating']\n",
    "    x = user_rating.drop(columns=['rating'])\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    length = len(y)\n",
    "    x_size = int(length * 0.8)\n",
    "    x_train, x_test = x.iloc[:x_size, :], x.iloc[x_size:, :]\n",
    "    y_train, y_test = y.iloc[:x_size], y.iloc[x_size:]\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=x_train.shape[1], activation='relu'),  # First hidden layer\n",
    "        Dense(32, activation='relu'),  # Second hidden layer\n",
    "        Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Evaluation for User ID {userId}:\")\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(f\"MSE: {train_mse:.4f}, R^2: {train_r2:.4f}\")\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"MSE: {test_mse:.4f}, R^2: {test_r2:.4f}\")\n",
    "\n",
    "    # Return MSE values for both sets\n",
    "    return train_mse, test_mse\n",
    "\n",
    "train_mse, test_mse = evaluate_model_for_user(8405, rating_films)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_model_df=one_hot_encoding_genre(df,movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'director'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'director'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/alexanderaujesky/Desktop/ADA/ada-2024-project-skibidata/alex_model.ipynb Cellule 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alexanderaujesky/Desktop/ADA/ada-2024-project-skibidata/alex_model.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Total_eval(\u001b[39m8405\u001b[39;49m,df,movies)\n",
      "File \u001b[0;32m~/Desktop/ADA/ada-2024-project-skibidata/helpers.py:368\u001b[0m, in \u001b[0;36mTotal_eval\u001b[0;34m(userId, df, movies)\u001b[0m\n\u001b[1;32m    365\u001b[0m all_genres_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(all_genres)\n\u001b[1;32m    366\u001b[0m all_genres_list\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39m(no genres listed)\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 368\u001b[0m unique_director \u001b[39m=\u001b[39m movies[\u001b[39m'\u001b[39;49m\u001b[39mdirector\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m    370\u001b[0m all_director_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(unique_director)\n\u001b[1;32m    371\u001b[0m all_director_list\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39mnobody\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'director'"
     ]
    }
   ],
   "source": [
    "Total_eval(8405,df,movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmdb_movie = pd.read_csv('TMdB_Movielens_combined/movies_metadata.csv', low_memory=False)\n",
    "df_tmdb_credits = pd.read_csv('TMdB_Movielens_combined/credits.csv')\n",
    "df_tmdb_keywords = pd.read_csv('TMdB_Movielens_combined/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>[{'id': 931, 'name': 'jealousy'}, {'id': 4290,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>[{'id': 10090, 'name': 'board game'}, {'id': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>[{'id': 1495, 'name': 'fishing'}, {'id': 12392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>[{'id': 818, 'name': 'based on novel'}, {'id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>[{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46414</th>\n",
       "      <td>439050</td>\n",
       "      <td>[{'id': 10703, 'name': 'tragic love'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46415</th>\n",
       "      <td>111109</td>\n",
       "      <td>[{'id': 2679, 'name': 'artist'}, {'id': 14531,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46416</th>\n",
       "      <td>67758</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46417</th>\n",
       "      <td>227506</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46418</th>\n",
       "      <td>461257</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           keywords\n",
       "0         862  [{'id': 931, 'name': 'jealousy'}, {'id': 4290,...\n",
       "1        8844  [{'id': 10090, 'name': 'board game'}, {'id': 1...\n",
       "2       15602  [{'id': 1495, 'name': 'fishing'}, {'id': 12392...\n",
       "3       31357  [{'id': 818, 'name': 'based on novel'}, {'id':...\n",
       "4       11862  [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...\n",
       "...       ...                                                ...\n",
       "46414  439050             [{'id': 10703, 'name': 'tragic love'}]\n",
       "46415  111109  [{'id': 2679, 'name': 'artist'}, {'id': 14531,...\n",
       "46416   67758                                                 []\n",
       "46417  227506                                                 []\n",
       "46418  461257                                                 []\n",
       "\n",
       "[46419 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#director, length, vote_average, release_date\n",
    "df_tmdb_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>title_format</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862.0</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>toy story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "      <td>[{'cast_id': 14, 'character': 'Woody (voice)',...</td>\n",
       "      <td>[{'credit_id': '52fe4284c3a36847f8024f49', 'de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "      <td>[{'cast_id': 1, 'character': 'Alan Parrish', '...</td>\n",
       "      <td>[{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "\n",
       "                               homepage      id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story   862.0  tt0114709                en   \n",
       "1                                   NaN  8844.0  tt0113497                en   \n",
       "\n",
       "  title_format                                           overview  ...  \\\n",
       "0    toy story  Led by Woody, Andy's toys live happily in his ...  ...   \n",
       "1      jumanji  When siblings Judy and Peter discover an encha...  ...   \n",
       "\n",
       "  runtime                                   spoken_languages    status  \\\n",
       "0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                                     tagline      title  video  vote_average  \\\n",
       "0                                        NaN  Toy Story  False           7.7   \n",
       "1  Roll the dice and unleash the excitement!    Jumanji  False           6.9   \n",
       "\n",
       "  vote_count                                               cast  \\\n",
       "0     5415.0  [{'cast_id': 14, 'character': 'Woody (voice)',...   \n",
       "1     2413.0  [{'cast_id': 1, 'character': 'Alan Parrish', '...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{'credit_id': '52fe4284c3a36847f8024f49', 'de...  \n",
       "1  [{'credit_id': '52fe44bfc3a36847f80a7cd1', 'de...  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmdb_movie.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n",
    "df_tmdb_credits.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n",
    "df_tmdb_keywords.drop_duplicates(subset=['id'], keep='first', inplace=True, ignore_index=True)\n",
    "# To merge we need to have a common type \n",
    "df_tmdb_movie['id'] = pd.to_numeric(df_tmdb_movie['id'], errors='coerce')\n",
    "df_tmdb_credits['id'] = pd.to_numeric(df_tmdb_credits['id'], errors='coerce')\n",
    "df_tmdb_keywords['id'] = pd.to_numeric(df_tmdb_keywords['id'], errors='coerce')\n",
    "df_tmdb_movie.dropna(subset=['id'], inplace=True)\n",
    "df_tmdb_credits.dropna(subset=['id'], inplace=True)\n",
    "df_tmdb_keywords.dropna(subset=['id'], inplace=True)\n",
    "df_tmp = pd.merge(df_tmdb_movie, df_tmdb_credits, on='id', how='inner')\n",
    "df_tmp = df_tmp.rename(columns={\"original_title\": \"title_format\"})\n",
    "df_tmp['title_format']=df_tmp['title_format'].apply(\n",
    "    lambda x : x.lower()\n",
    ")\n",
    "df_tmp.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/3334071302.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_columns['director'] = selected_columns['crew'].apply(get_director)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/3334071302.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_columns.drop(columns='crew', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime</th>\n",
       "      <th>director</th>\n",
       "      <th>movieId</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>1</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>2</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>3</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>4</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>5</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18617</th>\n",
       "      <td>705.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>7502</td>\n",
       "      <td>Action|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18618</th>\n",
       "      <td>60.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>27627</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18619</th>\n",
       "      <td>89.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>121821</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18620</th>\n",
       "      <td>143.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>96704</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18621</th>\n",
       "      <td>51.0</td>\n",
       "      <td>nobody</td>\n",
       "      <td>121320</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18622 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      runtime director movieId                                       genres\n",
       "0        81.0   nobody       1  Adventure|Animation|Children|Comedy|Fantasy\n",
       "1       104.0   nobody       2                   Adventure|Children|Fantasy\n",
       "2       101.0   nobody       3                               Comedy|Romance\n",
       "3       127.0   nobody       4                         Comedy|Drama|Romance\n",
       "4       106.0   nobody       5                                       Comedy\n",
       "...       ...      ...     ...                                          ...\n",
       "18617   705.0   nobody    7502                             Action|Drama|War\n",
       "18618    60.0   nobody   27627                                Drama|Romance\n",
       "18619    89.0   nobody  121821                                Drama|Romance\n",
       "18620   143.0   nobody   96704                                        Drama\n",
       "18621    51.0   nobody  121320                                        Drama\n",
       "\n",
       "[18622 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = df_tmp[['title_format', 'runtime', 'crew']]\n",
    "selected_columns['director'] = selected_columns['crew'].apply(get_director)\n",
    "selected_columns.drop(columns='crew', inplace=True)\n",
    "test_merge=pd.merge(selected_columns,movies, on='title_format',how='inner')\n",
    "test_merge[test_merge['director'].isna()]='nobody'\n",
    "director_counts = test_merge['director'].value_counts()\n",
    "test_merge['director'] = test_merge['director'].apply(\n",
    "    lambda x: x if director_counts[x] > 10 else 'nobody'\n",
    ")\n",
    "test_merge.drop(columns=['title_format','title'], inplace=True)\n",
    "test_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_eval_test(userId,df,movies) :\n",
    "    \"\"\"\n",
    "    Creates one hot encoded features per unique genre \n",
    "\n",
    "    \"\"\"\n",
    "    # Merges df and movies\n",
    "    mini_df=df[df['userId']==userId]\n",
    "    rating_films=pd.merge(movies, mini_df, on='movieId', how='inner')\n",
    "    rating_films.drop(columns='movieId',inplace=True)\n",
    "    # Create list of unique_genres\n",
    "    unique_genres = movies['genres'].unique()\n",
    "    all_genres = set('|'.join(unique_genres).split('|'))\n",
    "\n",
    "    all_genres_list = list(all_genres)\n",
    "    all_genres_list.remove('(no genres listed)')\n",
    "    \n",
    "    unique_director = movies['director'].unique()\n",
    "\n",
    "    all_director_list = list(unique_director)\n",
    "    all_director_list.remove('nobody')\n",
    "    # Create a one hot encoding for each genre\n",
    "    for genre in all_genres_list:\n",
    "        rating_films[genre] = rating_films['genres'].apply(lambda x: genre in x.split('|'))\n",
    "    # Create a one hot encoding for each director\n",
    "    for director in all_director_list:\n",
    "        rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
    "    rating_films.drop(columns=['director','genres','timestamp'],inplace=True)\n",
    "    return rating_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n",
      "/var/folders/bz/jj60x6y164ldxfdjhqsnv6dm0000gn/T/ipykernel_69795/1370888562.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rating_films[director] = rating_films['director'].apply(lambda x: director in x)\n"
     ]
    }
   ],
   "source": [
    "testing=Total_eval_test(200,df,test_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_user_test(userId, data):\n",
    "    \"\"\"\n",
    "    Generates training and test splits for a given user ID, trains a neural network,\n",
    "    and evaluates the model to return MSE and R^2 for both training and test sets.\n",
    "    \"\"\"\n",
    "    # Filter and preprocess data for the given user\n",
    "    user_rating = data[data['userId'] == userId]\n",
    "    user_rating = user_rating.sample(frac=1)  # Shuffle data\n",
    "    y = user_rating['rating'].astype('float32')\n",
    "    x = user_rating.drop(columns='rating').astype('float32')\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    length = len(y)\n",
    "    x_size = int(length * 0.8)\n",
    "    x_train, x_test = x.iloc[:x_size, :], x.iloc[x_size:, :]\n",
    "    y_train, y_test = y.iloc[:x_size], y.iloc[x_size:]\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = Sequential([\n",
    "        Dense(64, input_dim=x_train.shape[1], activation='relu'),  # First hidden layer\n",
    "        Dense(32, activation='relu'),  # Second hidden layer\n",
    "        Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=32, validation_split=0.3, verbose=1)\n",
    "\n",
    "    # Predict on training and test sets\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Evaluation for User ID {userId}:\")\n",
    "    print(\"Training Set Evaluation:\")\n",
    "    print(f\"MSE: {train_mse:.4f}, R^2: {train_r2:.4f}\")\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"MSE: {test_mse:.4f}, R^2: {test_r2:.4f}\")\n",
    "\n",
    "    # Return MSE values for both sets\n",
    "    return train_mse, test_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 217ms/step - loss: 18.7990 - mse: 18.7990 - val_loss: 7.5552 - val_mse: 7.5552\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.9480 - mse: 7.9480 - val_loss: 12.3506 - val_mse: 12.3506\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 9.7761 - mse: 9.7761 - val_loss: 1.1373 - val_mse: 1.1373\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 1.4080 - mse: 1.4080 - val_loss: 3.4169 - val_mse: 3.4169\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 4.3572 - mse: 4.3572 - val_loss: 4.1603 - val_mse: 4.1603\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 4.3740 - mse: 4.3740 - val_loss: 0.7248 - val_mse: 0.7248\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 1.1648 - mse: 1.1648 - val_loss: 1.9144 - val_mse: 1.9144\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.9887 - mse: 1.9887 - val_loss: 3.0057 - val_mse: 3.0057\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 2.8636 - mse: 2.8636 - val_loss: 1.0530 - val_mse: 1.0530\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0759 - mse: 1.0759 - val_loss: 0.4566 - val_mse: 0.4566\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.8217 - mse: 0.8217 - val_loss: 1.5605 - val_mse: 1.5605\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7211 - mse: 1.7211 - val_loss: 1.0965 - val_mse: 1.0965\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 1.1582 - mse: 1.1582 - val_loss: 0.3024 - val_mse: 0.3024\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.4634 - mse: 0.4634 - val_loss: 0.7467 - val_mse: 0.7467\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0287 - mse: 1.0287 - val_loss: 0.9933 - val_mse: 0.9933\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.1587 - mse: 1.1587 - val_loss: 0.3819 - val_mse: 0.3819\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5682 - mse: 0.5682 - val_loss: 0.4775 - val_mse: 0.4775\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6524 - mse: 0.6524 - val_loss: 0.7602 - val_mse: 0.7602\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.8285 - mse: 0.8285 - val_loss: 0.4365 - val_mse: 0.4365\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5160 - mse: 0.5160 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5743 - mse: 0.5743 - val_loss: 0.5570 - val_mse: 0.5570\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.7397 - mse: 0.7397 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5437 - mse: 0.5437 - val_loss: 0.3506 - val_mse: 0.3506\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4950 - mse: 0.4950 - val_loss: 0.4515 - val_mse: 0.4515\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5549 - mse: 0.5549 - val_loss: 0.3607 - val_mse: 0.3607\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4734 - mse: 0.4734 - val_loss: 0.3136 - val_mse: 0.3136\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4595 - mse: 0.4595 - val_loss: 0.3466 - val_mse: 0.3466\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5112 - mse: 0.5112 - val_loss: 0.3163 - val_mse: 0.3163\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4711 - mse: 0.4711 - val_loss: 0.3645 - val_mse: 0.3645\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4715 - mse: 0.4715 - val_loss: 0.3565 - val_mse: 0.3565\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4627 - mse: 0.4627 - val_loss: 0.3179 - val_mse: 0.3179\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4351 - mse: 0.4351 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4437 - mse: 0.4437 - val_loss: 0.3177 - val_mse: 0.3177\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4319 - mse: 0.4319 - val_loss: 0.3391 - val_mse: 0.3391\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4296 - mse: 0.4296 - val_loss: 0.3922 - val_mse: 0.3922\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.4664 - mse: 0.4664 - val_loss: 0.3507 - val_mse: 0.3507\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4383 - mse: 0.4383 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4374 - mse: 0.4374 - val_loss: 0.3341 - val_mse: 0.3341\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4396 - mse: 0.4396 - val_loss: 0.3312 - val_mse: 0.3312\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.4279 - val_mse: 0.4279\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4799 - mse: 0.4799 - val_loss: 0.4214 - val_mse: 0.4214\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4879 - mse: 0.4879 - val_loss: 0.3271 - val_mse: 0.3271\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4064 - mse: 0.4064 - val_loss: 0.3149 - val_mse: 0.3149\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4119 - mse: 0.4119 - val_loss: 0.3114 - val_mse: 0.3114\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4096 - mse: 0.4096 - val_loss: 0.3177 - val_mse: 0.3177\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4036 - mse: 0.4036 - val_loss: 0.3419 - val_mse: 0.3419\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4113 - mse: 0.4113 - val_loss: 0.3266 - val_mse: 0.3266\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.4036 - mse: 0.4036 - val_loss: 0.3136 - val_mse: 0.3136\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3990 - mse: 0.3990 - val_loss: 0.3283 - val_mse: 0.3283\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4053 - mse: 0.4053 - val_loss: 0.3555 - val_mse: 0.3555\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4015 - mse: 0.4015 - val_loss: 0.3271 - val_mse: 0.3271\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3985 - mse: 0.3985 - val_loss: 0.3327 - val_mse: 0.3327\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3902 - mse: 0.3902 - val_loss: 0.3537 - val_mse: 0.3537\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3851 - mse: 0.3851 - val_loss: 0.3849 - val_mse: 0.3849\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.3984 - mse: 0.3984 - val_loss: 0.3630 - val_mse: 0.3630\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3794 - mse: 0.3794 - val_loss: 0.3457 - val_mse: 0.3457\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4302 - mse: 0.4302 - val_loss: 0.3618 - val_mse: 0.3618\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4188 - mse: 0.4188 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3880 - mse: 0.3880 - val_loss: 0.4281 - val_mse: 0.4281\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.4170 - mse: 0.4170 - val_loss: 0.3477 - val_mse: 0.3477\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3679 - mse: 0.3679 - val_loss: 0.3419 - val_mse: 0.3419\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3829 - mse: 0.3829 - val_loss: 0.3461 - val_mse: 0.3461\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3678 - mse: 0.3678 - val_loss: 0.3878 - val_mse: 0.3878\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.3893 - mse: 0.3893 - val_loss: 0.4181 - val_mse: 0.4181\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.3843 - mse: 0.3843 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3681 - mse: 0.3681 - val_loss: 0.3488 - val_mse: 0.3488\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3747 - mse: 0.3747 - val_loss: 0.3531 - val_mse: 0.3531\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3497 - mse: 0.3497 - val_loss: 0.3778 - val_mse: 0.3778\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3640 - mse: 0.3640 - val_loss: 0.3479 - val_mse: 0.3479\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3569 - mse: 0.3569 - val_loss: 0.3381 - val_mse: 0.3381\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3600 - mse: 0.3600 - val_loss: 0.3457 - val_mse: 0.3457\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3441 - mse: 0.3441 - val_loss: 0.4305 - val_mse: 0.4305\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4077 - mse: 0.4077 - val_loss: 0.3481 - val_mse: 0.3481\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.3939 - mse: 0.3939 - val_loss: 0.3100 - val_mse: 0.3100\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3448 - mse: 0.3448 - val_loss: 0.3196 - val_mse: 0.3196\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.3480 - mse: 0.3480 - val_loss: 0.3093 - val_mse: 0.3093\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3391 - mse: 0.3391 - val_loss: 0.3078 - val_mse: 0.3078\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3456 - mse: 0.3456 - val_loss: 0.3090 - val_mse: 0.3090\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.3472 - mse: 0.3472 - val_loss: 0.3126 - val_mse: 0.3126\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3343 - mse: 0.3343 - val_loss: 0.3206 - val_mse: 0.3206\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.3303 - mse: 0.3303 - val_loss: 0.3492 - val_mse: 0.3492\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.3311 - mse: 0.3311 - val_loss: 0.3707 - val_mse: 0.3707\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3263 - mse: 0.3263 - val_loss: 0.3631 - val_mse: 0.3631\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.3234 - mse: 0.3234 - val_loss: 0.3874 - val_mse: 0.3874\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.3151 - mse: 0.3151 - val_loss: 0.4603 - val_mse: 0.4603\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3542 - mse: 0.3542 - val_loss: 0.4048 - val_mse: 0.4048\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.3046 - mse: 0.3046 - val_loss: 0.3932 - val_mse: 0.3932\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3887 - mse: 0.3887 - val_loss: 0.3882 - val_mse: 0.3882\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3665 - mse: 0.3665 - val_loss: 0.4084 - val_mse: 0.4084\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3217 - mse: 0.3217 - val_loss: 0.4951 - val_mse: 0.4951\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3768 - mse: 0.3768 - val_loss: 0.3464 - val_mse: 0.3464\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.3243 - mse: 0.3243 - val_loss: 0.4039 - val_mse: 0.4039\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3910 - mse: 0.3910 - val_loss: 0.3875 - val_mse: 0.3875\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3417 - mse: 0.3417 - val_loss: 0.5800 - val_mse: 0.5800\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3921 - mse: 0.3921 - val_loss: 0.3813 - val_mse: 0.3813\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3936 - mse: 0.3936 - val_loss: 0.4116 - val_mse: 0.4116\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3729 - mse: 0.3729 - val_loss: 0.4894 - val_mse: 0.4894\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.3492 - mse: 0.3492 - val_loss: 0.4307 - val_mse: 0.4307\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.2971 - mse: 0.2971 - val_loss: 0.3954 - val_mse: 0.3954\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.3740 - mse: 0.3740 - val_loss: 0.3729 - val_mse: 0.3729\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Evaluation for User ID 200:\n",
      "Training Set Evaluation:\n",
      "MSE: 0.3164, R^2: 0.3699\n",
      "\n",
      "Test Set Evaluation:\n",
      "MSE: 1.2004, R^2: 0.0875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31638274, 1.2003912)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model_for_user_test(200,testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>[{'id': 931, 'name': 'jealousy'}, {'id': 4290,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           keywords\n",
       "0  862  [{'id': 931, 'name': 'jealousy'}, {'id': 4290,..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=df_tmdb_keywords[df_tmdb_keywords['id']==862]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        2     3.5\n",
       "1       1       29     3.5\n",
       "2       1       32     3.5\n",
       "3       1       47     3.5\n",
       "4       1       50     3.5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_df=df.drop(columns='timestamp')\n",
    "micro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(m, u, r, delta_m, delta_u):\n",
    "    error = np.dot(m, u.T) - r \n",
    "    grad_m = 2 * error * u + 2 * delta_m * m  \n",
    "    grad_u = 2 * error * m + 2 * delta_u * u  \n",
    "    return grad_m, grad_u\n",
    "class Predictor:\n",
    "    def __init__(self, train_data):\n",
    "        # This is where you can do any preparation or 'training' if necessary.\n",
    "        pass\n",
    "\n",
    "    def __call__(self, test_data):\n",
    "        \"\"\"\n",
    "        Make predictions for the users and movies in the test dataset\n",
    "        (without looking at the ratings).\n",
    "\n",
    "        Inputs:\n",
    "            test_data: Dataset (with `n` entries)\n",
    "\n",
    "        Output:\n",
    "            predictions: np.array of floats, shape (`n`)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class MatrixFactorizationPredictor(Predictor):\n",
    "    def __init__(self, train_data, num_features=20, seed=1):\n",
    "        # Randomly initialize features for the users and the movies from N(0, 1)\n",
    "\n",
    "        # use this generator (https://numpy.org/doc/stable/reference/random/index.html)\n",
    "        # you are expected to use rng.normal() twice in this function to match the tests, once for movies, and then once for users\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        num_movies = np.max(train_data.movies) + 1\n",
    "        num_users = np.max(train_data.users) + 1\n",
    "        ####################################\n",
    "        ### ___ ENTER YOUR CODE HERE ___ ###\n",
    "        ####################################\n",
    "        \n",
    "        self.movies_=train_data['movie'].to_numpy()\n",
    "        self.users_=train_data['user'].to_numpy()\n",
    "        self.ratings_=train_data['rating'].to_numpy()\n",
    "        self.movie_features = rng.normal(loc=0.0, scale=1.0, size=(num_movies, num_features))\n",
    "        self.user_features = rng.normal(loc=0.0, scale=1.0, size=(num_users, num_features))\n",
    "        \n",
    "        # Normally, you should train the model here, but we will skip this\n",
    "        # for now, to be able to take it step-by-step.\n",
    "    def gradient(m, u, r, delta_m, delta_u):\n",
    "        error = np.dot(m, u.T) - r \n",
    "        grad_m = 2 * error * u + 2 * delta_m * m  \n",
    "        grad_u = 2 * error * m + 2 * delta_u * u  \n",
    "        return grad_m, grad_u\n",
    "    def train(self, movie,user,rating, weight_decay_movie=0.3,weight_decay_user=0.3,learning_rate=0.02):\n",
    "        \"\"\"\n",
    "        Train the model using Stochastic Gradient Descent.\n",
    "        \"\"\"\n",
    "         \n",
    "        movie_feature_vector = self.movie_features[movie, :]\n",
    "        user_feature_vector = self.user_features[user, :]\n",
    "                \n",
    "        grad_m, grad_u = gradient(movie_feature_vector, user_feature_vector, rating, weight_decay_movie, weight_decay_user)\n",
    "                    \n",
    "        # Update the feature vectors\n",
    "        self.movie_features[movie, :] -= learning_rate * grad_m\n",
    "        self.user_features[user, :] -= learning_rate * grad_u      \n",
    "    def train_user(self,user, weight_decay_user=0.3, verbose = False):\n",
    "        \"\"\"\n",
    "        Train the model using Stochastic Gradient Descent.\n",
    "        \"\"\"\n",
    "        mask = self.users_ == user\n",
    "        user_movies = self.movies_[mask]\n",
    "        ratings=self.ratings_[mask]\n",
    "        M = self.movie_features[user_movies]\n",
    "        A = M.T @ M + weight_decay_user * np.eye(M.shape[1])  # Matrix for solving\n",
    "        b = M.T @ ratings\n",
    "        if verbose:\n",
    "            print(\"Avant\" )\n",
    "            print(self.user_features[user][0] )\n",
    "            print()\n",
    "\n",
    "        self.user_features[user] = np.linalg.solve(A, b)\n",
    "        if verbose:\n",
    "            print(\"Après\" )\n",
    "            print(self.user_features[user][0] )\n",
    "            print()\n",
    "        \n",
    "    def train_movies(self, movie, weight_decay_movie=0.3, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model using Stochastic Gradient Descent.\n",
    "        \"\"\"\n",
    "         \n",
    "        mask = self.movies_ == movie\n",
    "        movie_users = self.users_[mask]\n",
    "        ratings=self.ratings_[mask]\n",
    "\n",
    "        U = self.user_features[movie_users]\n",
    "        A = U.T @ U + weight_decay_movie * np.eye(U.shape[1])  # Matrix for solving\n",
    "        b = U.T @ ratings\n",
    "        \n",
    "        \n",
    "        self.movie_features[movie] = np.linalg.solve(A, b)    \n",
    "        \n",
    "        if verbose:\n",
    "            print(self.movie_features[movie][0])\n",
    "            \n",
    "    def __call__(self, test_data):\n",
    "        \"\"\"\n",
    "        Predict the rating of a user/movie pair as the dot-product\n",
    "        of representation vectors of the user and the movie.\n",
    "\n",
    "        >>> train_data = Dataset(np.array([0, 0, 1, 1, 2]), np.array([1, 2, 3, 4, 5]), np.array([4.0, 1.0, 1.0, 2.0, 1.0]))\n",
    "        >>> test_data = Dataset(np.array([0, 1, 2, 1]), np.array([1, 2, 2, 0]), np.array([1.0, 2.0, 2.5, 5.0]))\n",
    "        >>> mean_predictor = MatrixFactorizationPredictor(train_data)\n",
    "        >>> mean_predictor(test_data)  # the factorization is not yet optimized here\n",
    "        array([ 2.62654714, -2.89866225,  0.70909287,  5.29901482])\n",
    "        \"\"\"\n",
    "        predictions=[]\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "        \n",
    "        for i in range(len(test_data['movie'])) :\n",
    "            value_m=test_data.movies[i]\n",
    "            value_u=test_data.users[i]\n",
    "            pred=np.dot(self.movie_features[value_m,:],self.user_features[value_u,:].T)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)\n",
    "    \n",
    "def split_dataset(dataset, p_test=0.1, seed=1):\n",
    "    \"\"\"\n",
    "    Split a dataset randomly into a train and a test part\n",
    "\n",
    "    Inputs:\n",
    "        dataset: Dataset\n",
    "        p_test: float\n",
    "            propability (0 < p_test < 1) for a data point to go into the test set\n",
    "        seed: integer\n",
    "\n",
    "    Returns:\n",
    "        train_dataset: Dataset\n",
    "        test_dataset: Dataset\n",
    "\n",
    "    >>> split_dataset(Dataset(np.array([0, 0]), np.array([1, 0]), np.array([2.0, 1.0])), p_test=0)\n",
    "    (Dataset(movies=array([0, 0]), users=array([1, 0]), ratings=array([2., 1.])), Dataset(movies=array([], dtype=int64), users=array([], dtype=int64), ratings=array([], dtype=float64)))\n",
    "\n",
    "    >>> split_dataset(Dataset(np.array([0, 0]), np.array([1, 0]), np.array([2.0, 1.0])), p_test=1)\n",
    "    (Dataset(movies=array([], dtype=int64), users=array([], dtype=int64), ratings=array([], dtype=float64)), Dataset(movies=array([0, 0]), users=array([1, 0]), ratings=array([2., 1.])))\n",
    "    \"\"\"\n",
    "    # use this generator (https://numpy.org/doc/stable/reference/random/index.html)\n",
    "    # you should use rng.uniform() once inside this function to match the automatic test case\n",
    "    # Use the random number generator with the specified seed\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    random_values = rng.uniform(0, 1, len(dataset.movies))\n",
    "\n",
    "    test_mask = random_values < p_test\n",
    "\n",
    "    test_data = Dataset(\n",
    "        movies=dataset.movies[test_mask],\n",
    "        users=dataset.users[test_mask],\n",
    "        ratings=dataset.ratings[test_mask]\n",
    "    )\n",
    "    \n",
    "    train_data = Dataset(\n",
    "        movies=dataset.movies[~test_mask],\n",
    "        users=dataset.users[~test_mask],\n",
    "        ratings=dataset.ratings[~test_mask]\n",
    "    )\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e060fb6a235d6702d70df6454c98d2cb490f8716e1a8dc9eca7c491f2c17490"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
